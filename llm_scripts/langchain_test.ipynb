{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initiating the OpenAI LLM with API key\n",
    "llm = OpenAI()\n",
    "\n",
    "# Query the model\n",
    "response = llm.invoke(\"What is the tallest building in the world?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initiating the chat model with API key\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Defines a context and query using SystemMessage and HumanMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a math tutor who provides answers with a bit of sarcasm.\"),\n",
    "    HumanMessage(content=\"What is the square of 2?\"),\n",
    "]\n",
    " \n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the template string\n",
    "email_template = \"\"\"Create an invitation email to the recipinet that is {recipient_name} \\\n",
    " for an event that is {event_type} in a language that is {language} \\\n",
    " Mention the event location that is {event_location} \\\n",
    " and event date that is {event_date}. \\\n",
    " Also write few sentences about the event description that is {event_description} \\\n",
    " in style that is {style} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LangChain modules\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(email_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "message = prompt_template.format_prompt(\n",
    "    style = \"enthusiastic tone\",\n",
    "    language = \"American english\",\n",
    "    recipient_name=\"John\",\n",
    "    event_type=\"product launch\",\n",
    "    event_date=\"January 15, 2024\",\n",
    "    event_location=\"Grand Ballroom, City Center Hotel\",\n",
    "    event_description=\"an exciting unveiling of our latest innovations\"\n",
    "    )\n",
    "\n",
    "response = llm.invoke(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LangChain modules\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"review\": \"I absolutely love this product! It exceeded my expectations.\",\n",
    "    \"sentiment\": \"Positive\"\n",
    "  },\n",
    "  {\n",
    "    \"review\": \"I'm really disappointed with the quality of this item. It didn't meet my needs.\",\n",
    "    \"sentiment\": \"Negative\"\n",
    "  },\n",
    "  {\n",
    "    \"review\": \"The product is okay, but there's room for improvement.\",\n",
    "    \"sentiment\": \"Neutral\"\n",
    "  }\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "                        input_variables=[\"review\", \"sentiment\"], \n",
    "                        template=\"Review: {review}\\n{sentiment}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Review: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "message = prompt.format(input=\"The machine worked okay without much trouble.\")\n",
    "\n",
    "response = chat.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LangChain modules\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser, CommaSeparatedListOutputParser\n",
    "\n",
    "llm = OpenAI()\n",
    "parser_dateTime = DatetimeOutputParser()\n",
    "parser_List = CommaSeparatedListOutputParser()\n",
    "\n",
    "# creating our prompt template\n",
    "template = \"\"\"Provide the response in format {format_instructions} \n",
    "            to the user's question {question}\"\"\"\n",
    "\n",
    "prompt_dateTime = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": parser_dateTime.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt_List = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": parser_List.get_format_instructions()},\n",
    ")\n",
    "\n",
    "print(llm.predict(text = prompt_dateTime.format(question=\"When was the first iPhone launched?\")))\n",
    "print(llm.predict(text = prompt_List.format(question=\"What are the four famous chocolate brands?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "model = OpenAI()\n",
    "\n",
    "class Author(BaseModel):\n",
    "    number: int = Field(description=\"number of books written by the author\")\n",
    "    books:  List[int] = Field(description=\"list of books they wrote\")\n",
    "\n",
    "user_query = \"Generate the books written by Dan Brown.\"\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Author)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "my_prompt = prompt.format_prompt(query=user_query)\n",
    "\n",
    "output = model(my_prompt.to_string())\n",
    "\n",
    "print(output_parser.parse(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# defining the LLM model\n",
    "llm = OpenAI(temperature=0.0)\n",
    "\n",
    "# creating the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author of the book {book}?\",\n",
    ")\n",
    "\n",
    "# creating the chain\n",
    "chain = LLMChain(llm=llm, \n",
    "                prompt=prompt_template, \n",
    "                verbose=True)\n",
    "\n",
    "# calling the chain\n",
    "print(chain.run(\"The Da Vinci Code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0.)\n",
    "\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author who wrote the {book}?\"\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"author_name\"],\n",
    "    template=\"Write a 50-word biography for the following author:{author_name}\"\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "simple_seq_chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "simple_seq_chain.run(\"The Da Vincci Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0.)\n",
    "\n",
    "biography = \"He is an American author of thriller fiction, best known for his Robert Langdon series. \\\n",
    "          He has sold over 200 million copies of his books, which have been translated into 56 \\\n",
    "          languages. His other works include Angels & Demons, The Lost Symbol, Inferno, and Origin. \\\n",
    "          He is a New York Times best-selling author and has been awarded numerous awards for his \\\n",
    "          writing.\"\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this biography in one sentence:\\n\\n{biography}\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"one_line_biography\")\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(\n",
    "    \"Can you tell the author's name in this biography:\\n\\n{one_line_biography}\"\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"author_name\")\n",
    "\n",
    "prompt_3 = ChatPromptTemplate.from_template(\n",
    "    \"Tell the name of the highest selling book of this author: \\n\\n{author_name}\"\n",
    ")\n",
    "\n",
    "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"book\")\n",
    "\n",
    "prompt_4 = ChatPromptTemplate.from_template(\n",
    "     \"Write a follow-up response to the following \"\n",
    "    \"summary of the highest-selling book of the author:\"\n",
    "    \"\\n\\nAuthor: {author_name}\\n\\nBook: {book}\"\n",
    ")\n",
    "\n",
    "chain_4 = LLMChain(llm=llm, prompt=prompt_4, output_key=\"summary\")\n",
    "\n",
    "final_chain = SequentialChain(\n",
    "    chains = [chain_1, chain_2, chain_3, chain_4],\n",
    "    input_variables=[\"biography\"],\n",
    "    output_variables=[\"one_line_biography\", \"author_name\", \"summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(final_chain(biography))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining prompt templates for the destination chains \n",
    "shipping_template = \"\"\"You are the shipping manager of a company. \\\n",
    "As a shipping customer service agent, respond to a customer inquiry about the current status \\\n",
    "and estimated delivery time of their package. Include details about the \\\n",
    "shipping route and any potential delays, providing a comprehensive and reassuring response. \\\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "billing_template = \"\"\"You are the billing manager of a company. \\\n",
    "Address a customer's inquiry regarding an unexpected charge on their account.\\\n",
    "Explain the nature of the charge and any relevant billing policies, and promptly resolve \\\n",
    "the concern to ensure customer satisfaction. Additionally, offer guidance on how the \\\n",
    "customer can monitor and manage their billing preferences moving forward.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "technical_template = \"\"\"You are very good at understanding the technology of your company's product. \\\n",
    "Assist a customer experiencing issues with a software application. \\\n",
    "Walk them through troubleshooting steps, provide clear instructions \\\n",
    "on potential solutions, and ensure the customer feels supported\\\n",
    "throughout the process. Additionally, offer guidance on preventive \\\n",
    "measures to minimize future technical issues and optimize their \\\n",
    "experience with the software.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the prompt templates in prompt_infos\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"Shipping\",\n",
    "        \"description\": \"Good for answering questions about shipping issues of a product\",\n",
    "        \"prompt_template\": shipping_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Billing\",\n",
    "        \"description\": \"Good for answering questions regarding billing issues of a product\",\n",
    "        \"prompt_template\": billing_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical\",\n",
    "        \"description\": \"Good for answering questions regarding technical issues of a product\",\n",
    "        \"prompt_template\": technical_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info['name']\n",
    "    prompt_template = p_info['prompt_template']\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f'{p[\"name\"]}: {p[\"description\"]}' for p in prompt_infos]\n",
    "destination_str = '\\n'.join(destinations)\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "final_chain = MultiPromptChain(router_chain=router_chain,\n",
    "                               destination_chains=destination_chains,\n",
    "                               default_chain=default_chain,\n",
    "                               verbose=True)\n",
    "\n",
    "print(final_chain.run(\"The package was supposed to arrive last week and I haven't received it yet. I want to cancel my order NOW!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0.)\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory.save_context({\"input\": \"Alex is a 9-year old boy.\"}, \n",
    "                    {\"output\": \"Hello Alex! How can I assist you today?\"})\n",
    "memory.save_context({\"input\": \"Alex likes to play football\"}, \n",
    "                    {\"output\": \"That's great to hear! \"})\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "print(conversation.predict(input=\"How old is Alex?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"d5e76eb36da21d4fdf5f2ae41c7397326817bbfbf93d9a60af431ddbd18338fe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "print(agent.run(\"What is the current population of the world, and calculate the percentage change compared to the population five years ago\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature=0.)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "tools = load_tools([\"serpapi\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n",
    "\n",
    "agent.run(\"Hi, my name is Alex, and I live in the New York City.\")\n",
    "agent.run(\"My favorite game is basketball.\")\n",
    "\n",
    "print(agent.run(\"Give me the list of stadiums to watch a basketball game in my city today. Also give the teams that are playing.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_document = TextLoader(\"my_document.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    length_function = len,\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 10,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([input_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "query = \"WRITE_YOUR_QUERY_HERE\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_vector \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\u001b[38;5;241m.\u001b[39membed_query(\u001b[43mquery\u001b[49m)\n\u001b[1;32m      3\u001b[0m docs \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39msimilarity_search_by_vector(embedding_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "\n",
    "docs = db.similarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma \n",
    "\n",
    "db = Chroma.from_texts(texts, embedding)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory \n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path ./3DPrinter_Manual.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./3DPrinter_Manual.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mypdf \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:157\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:100\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path ./3DPrinter_Manual.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./3DPrinter_Manual.pdf\")\n",
    "mypdf = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mypdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m document_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m document_splitter\u001b[38;5;241m.\u001b[39msplit_documents(\u001b[43mmypdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mypdf' is not defined"
     ]
    }
   ],
   "source": [
    "document_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=70)\n",
    "docs = document_splitter.split_documents(mypdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m      2\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m my_database \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m----> 5\u001b[0m     documents\u001b[38;5;241m=\u001b[39m\u001b[43mdocs\u001b[49m,\n\u001b[1;32m      6\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m      7\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m retaining_memory \u001b[38;5;241m=\u001b[39m ConversationBufferWindowMemory(memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m question_answering \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(llm, retriever\u001b[38;5;241m=\u001b[39mmy_database\u001b[38;5;241m.\u001b[39mas_retriever(), memory\u001b[38;5;241m=\u001b[39mretaining_memory)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "persist_directory = 'db'\n",
    "\n",
    "my_database = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embeddings=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "retaining_memory = ConversationBufferWindowMemory(memory_key='chat_history', k=5, return_messages=True)\n",
    "\n",
    "question_answering = ConversationalRetrievalChain.from_llm(llm, retriever=my_database.as_retriever(), memory=retaining_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Entery your query: \")\n",
    "    if question == 'exit':\n",
    "        break\n",
    "    result = question_answering({\"question\": \"Answer only in the context of the document provided.\" + question})\n",
    "    print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
