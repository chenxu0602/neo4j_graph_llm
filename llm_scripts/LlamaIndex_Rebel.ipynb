{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    'text2text-generation',\n",
    "    model='Babelscape/rebel-large',\n",
    "    tokenizer='Babelscape/rebel-large',\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to parse the generated text and extract the triplets\n",
    "# Rebel outputs a specific format. This code is mostly copied from the model card!\n",
    "\n",
    "def clean_triplets(input_text, triplets):\n",
    "    \"\"\"Sometimes the model hallucinates, so we filter out entities\n",
    "       not present in the text\"\"\"\n",
    "    text = input_text.lower()\n",
    "    clean_triplets = []\n",
    "    for triplet in triplets:\n",
    "\n",
    "        if (triplet[\"head\"] == triplet[\"tail\"]):\n",
    "            continue\n",
    "\n",
    "        head_match = re.search(\n",
    "            r'\\b' + re.escape(triplet[\"head\"].lower()) + r'\\b', text)\n",
    "        if head_match:\n",
    "            head_index = head_match.start()\n",
    "        else:\n",
    "            head_index = text.find(triplet[\"head\"].lower())\n",
    "\n",
    "        tail_match = re.search(\n",
    "            r'\\b' + re.escape(triplet[\"tail\"].lower()) + r'\\b', text)\n",
    "        if tail_match:\n",
    "            tail_index = tail_match.start()\n",
    "        else:\n",
    "            tail_index = text.find(triplet[\"tail\"].lower())\n",
    "\n",
    "        if ((head_index == -1) or (tail_index == -1)):\n",
    "            continue\n",
    "\n",
    "        clean_triplets.append((triplet[\"head\"], triplet[\"type\"], triplet[\"tail\"]))\n",
    "\n",
    "    return clean_triplets\n",
    "\n",
    "def extract_triplets(input_text):\n",
    "    text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])[0]\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail':object_.strip()})\n",
    "    clean = clean_triplets(input_text, triplets)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "username = \"neo4j\"\n",
    "password = \"LimeStardom6J\"\n",
    "url = \"bolt://localhost:7687\"\n",
    "database = \"neo4j\"\n",
    "\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/s0dqxvt557l2gr1n1d9z_r7m0000gn/T/ipykernel_76100/4076674629.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  WikipediaReader = download_loader(\"WikipediaReader\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-wikipedia in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-readers-wikipedia) (0.10.5)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.25.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.7.2)\n",
      "Requirement already satisfied: pandas in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.1.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (10.1.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.5.2)\n",
      "Requirement already satisfied: sniffio in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.3.0)\n",
      "Requirement already satisfied: certifi in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (2.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/chenxu/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-wikipedia) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, ServiceContext\n",
    "from llama_index.core.readers import download_loader\n",
    "\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(\n",
    "    pages=[\"The Elder Scrolls V: Skyrim\"], auto_suggest=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/s0dqxvt557l2gr1n1d9z_r7m0000gn/T/ipykernel_76100/2653091649.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, chunk_size=256)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# rebel supports up to 512 input tokens, but shorter sequences also work well\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=256)\n",
    "\n",
    "# This can take some times\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    kg_triplet_extract_fn=extract_triplets,\n",
    "    service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<class 'llama_index.graph_stores.neo4j.base.Neo4jGraphStore'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnowledgeGraphQueryEngine\n\u001b[0;32m----> 3\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mKnowledgeGraphQueryEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m#    service_context=service_context,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefresh_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/torch/lib/python3.11/site-packages/llama_index/core/query_engine/knowledge_graph_query_engine.py:93\u001b[0m, in \u001b[0;36mKnowledgeGraphQueryEngine.__init__\u001b[0;34m(self, llm, storage_context, graph_query_synthesis_prompt, graph_response_answer_prompt, refresh_schema, verbose, response_synthesizer, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Get Graph Store Type\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_store_type \u001b[38;5;241m=\u001b[39m \u001b[43mGRAPH_STORE_CLASS_TO_GRAPH_STORE_TYPE\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Get Graph schema\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store\u001b[38;5;241m.\u001b[39mget_schema(refresh\u001b[38;5;241m=\u001b[39mrefresh_schema)\n",
      "\u001b[0;31mKeyError\u001b[0m: <class 'llama_index.graph_stores.neo4j.base.Neo4jGraphStore'>"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    refresh_schema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the highest point in Skyrim?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
